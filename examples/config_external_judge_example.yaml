# Example Configuration for Using External Judge with CLEAR
# This config demonstrates how to use a custom external judge instead of LLM evaluation

# Basic settings
run_name: external_judge_demo
data_path: "path/to/your/data.csv"
output_dir: "results/external_judge_demo/"

# Task configuration
task: general  # or 'math', 'rag', etc.
perform_generation: false  # Set to true if you need to generate responses first
is_reference_based: true  # External judge will compare response to ground_truth

# Column mappings (adjust to match your CSV columns)
model_input_column: model_input
model_output_column: response
reference_column: ground_truth
qid_column: id

# External Judge Configuration
judge_type: external  # Use 'external' instead of 'llm'
external_judge_path: examples/custom_judges/exact_match_judge.py
external_judge_function: evaluate  # Function name in the judge file
external_judge_config: {}  # Additional config for your judge (if needed)

# Analysis settings
success_threshold: 0.91  # Minimum score to be considered successful
max_workers: 10  # Parallel processing workers
resume_enabled: true  # Reuse cached intermediate results

# Issue discovery settings
perform_clustering: true
max_shortcomings: 15
min_shortcomings: 3
use_full_text_for_analysis: false
max_eval_text_for_synthesis: 1000

# Provider settings (still needed for issue synthesis/clustering)
# Even with external judge, LLM is used for aggregating issues
provider: openai  # or watsonx, rits
# Make sure to set appropriate environment variables for your provider

# Made with Bob
